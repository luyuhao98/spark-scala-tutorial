{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03: WordCount - Alternative Implementations of WordCount\n",
    "\n",
    "This exercise also implements *Word Count*, but it uses a slightly simpler approach. It also shows one way to make the code more configurable. We'll define variables for the input and output locations. The corresponding Spark program, [WordCount3.scala](https://github.com/deanwampler/spark-scala-tutorial/blob/master/src/main/scala/sparktutorial/WordCount3.scala) uses a utility library to support command-line arguments. (The library demonstrates some idiomatic, but fairly advanced Scala code, but you can ignore the details and just use it.) \n",
    "\n",
    "Next, instead of using the old approach of creating a `SparkContext`, like we did in <a href=\"02_WordCount.ipynb\" target=\"02_WC\">02_WordCount</a>, we'll use the now recommended approach of creating a `SparkSession` and extracting the `SparkContext` from it when needed. Finally, we'll also use [Kryo Serialization](http://spark.apache.org/docs/latest/tuning.html), which provides better compression and therefore better utilization of memory and network bandwidth (not that we really need it for this small dataset...).\n",
    "\n",
    "This version also does some data cleansing to improve the results. The sacred text files included in the `data` directory, such as `kjvdat.txt` are actually formatted records of the form:\n",
    "\n",
    "```text\n",
    "book|chapter#|verse#|text\n",
    "```\n",
    "\n",
    "That is, pipe-separated fields with the book of the Bible (e.g., Genesis, but abbreviated \"Gen\"), the chapter and verse numbers, and then the verse text. We just want to count words in the verses, although including the book names wouldn't change the results significantly. (Now you can figure out the answer to one of the exercises in the previous example...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The corresponding Spark program is [WordCount2.scala](https://github.com/deanwampler/spark-scala-tutorial/blob/master/src/main/scala/sparktutorial/WordCount2.scala). It shows you how to structure a Spark program, including imports and one way to construct the required [SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)\n",
    "\n",
    "We'll use the KJV Bible text again. Subsequent exercises will add the ability to specify different input sources using command-line arguments.\n",
    "\n",
    "This time, we'll define variables for the input and output data locations. So, if you want to use the same code to process different data, just edit the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://3186ab558fdd:4040\n",
       "SparkContext available as 'sc' (version = 3.2.1, master = local[*], app id = local-1655571501305)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "in: String = ../data/kjvdat.txt\n",
       "out: String = output/kjv-wc3\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val in  = \"../data/kjvdat.txt\"    // input file\n",
    "val out = \"output/kjv-wc3\"        // output location (directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, we read the text file, convert the lines to lower case, and tokenize into works. Let's start this time by defining a helper method that handles the record format; it will strip off the leading `book|chapter#|verse#|`, leaving just the verse text. we do this by splitting the line on `|`, which returns an `Array[String]`. Then we keep the last element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toText: (str: String)String\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def toText(str: String): String = {\n",
    "  val ary = str.toLowerCase.split(\"\\\\s*\\\\|\\\\s*\")\n",
    "  if (ary.length > 0) ary.last else \"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2] at map at <console>:26\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input = sc.textFile(in).map(line => toText(line))  // could also write ...map(toText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that if you will read `input` several times, then cache the data so Spark doesn't reread from disk each time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[2] at map at <console>:31"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in the beginning god created the heaven and the earth.~\n",
      "and the earth was without form, and void; and darkness was upon the face of the deep. and the spirit of god moved upon the face of the waters.~\n",
      "in\n",
      "=============\n",
      "the\n",
      "=============\n",
      "beginning\n",
      "=============\n",
      "god\n",
      "=============\n",
      "created\n",
      "=============\n",
      "the\n",
      "=============\n",
      "heaven\n",
      "=============\n",
      "and\n",
      "=============\n",
      "the\n",
      "=============\n",
      "earth\n",
      "=============\n",
      "and\n",
      "=============\n",
      "the\n",
      "=============\n",
      "earth\n",
      "=============\n",
      "was\n",
      "=============\n",
      "without\n",
      "=============\n",
      "form\n",
      "=============\n",
      "and\n",
      "=============\n",
      "void\n",
      "=============\n",
      "and\n",
      "=============\n",
      "darkness\n",
      "=============\n",
      "was\n",
      "=============\n",
      "upon\n",
      "=============\n",
      "the\n",
      "=============\n",
      "face\n",
      "=============\n",
      "of\n",
      "=============\n",
      "the\n",
      "=============\n",
      "deep\n",
      "=============\n",
      "and\n",
      "=============\n",
      "the\n",
      "=============\n",
      "spirit\n",
      "=============\n",
      "of\n",
      "=============\n",
      "god\n",
      "=============\n",
      "moved\n",
      "=============\n",
      "upon\n",
      "=============\n",
      "the\n",
      "=============\n",
      "face\n",
      "=============\n",
      "of\n",
      "=============\n",
      "the\n",
      "=============\n",
      "waters\n",
      "=============\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "res7: Array[Unit] = Array((), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), ())\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.take(2).map(println)\n",
    "input.take(2)\n",
    "  .flatMap(line => line.split(\"\"\"[^\\p{IsAlphabetic}]+\"\"\")).map(line => {println(line) ;\n",
    "                                                                        println(\"=============\")} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is one long statement that is similar to what we saw in _02_WordCount_, but with a few differences. \n",
    "\n",
    "Take the `input` and:\n",
    "1. Split each line on non-alphanumeric characters (a crude form of tokenization). `flatMap` \"flattens\" each array returned into a since `RDD` of words\n",
    "2. Use `countByValue` to treat each word as a key, then count all the keys. This returns a Scala `Map[String,Long]` to the driver, so be careful about `OutOfMemory` (`OOM`) errors for very large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wc1: scala.collection.Map[String,Long] = Map(onam -> 4, professed -> 2, confesseth -> 3, brink -> 6, youthful -> 1, healings -> 1, sneezed -> 1, forgotten -> 46, precious -> 76, inkhorn -> 3, exorcists -> 1, derided -> 2, eatest -> 3, lover -> 4, centurion -> 21, plentiful -> 4, pasture -> 20, sargon -> 1, speaker -> 2, terrible -> 52, lion -> 104, rate -> 5, zorites -> 1, mole -> 1, lights -> 10, arimathaea -> 4, spokes -> 1, rage -> 18, submitted -> 3, engraver -> 3, ahava -> 3, ferret -> 1, snow -> 24, desolate -> 148, laughing -> 1, jabbok -> 7, shuttle -> 1, arodites -> 1, michael -> 15, darkened -> 19, camest -> 28, abhorrest -> 2, beheld -> 53, looks -> 5, alpha -> 4, crieth -> 17, adders -> 1, holpen -> 5, chargeable -> 5, galatians -> 1, jezaniah -> 2, pudens -> 1, jeremiah -> ...\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wc1 = input\n",
    "  .flatMap(line => line.split(\"\"\"[^\\p{IsAlphabetic}]+\"\"\"))\n",
    "  .countByValue()     // Returns a Scala Map[T, Long] to the driver; no more RDD!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's convert back to an `RDD` for output. We'll use one partition (the `1` argument you'll see below). To do this, we first convert to a comma-separated string. Note that calling `map` on a scala `Map` passes two-element tuples for the key-value pairs to the function. We extract the first and second elements with the `_1` and `_2` methods, respectively, with which we format strings for output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wc2pre: Iterable[String] = List(onam,4, professed,2, confesseth,3, brink,6, youthful,1, healings,1, sneezed,1, forgotten,46, precious,76, inkhorn,3, exorcists,1, derided,2, eatest,3, lover,4, centurion,21, plentiful,4, pasture,20, sargon,1, speaker,2, terrible,52, lion,104, rate,5, zorites,1, mole,1, lights,10, arimathaea,4, spokes,1, rage,18, submitted,3, engraver,3, ahava,3, ferret,1, snow,24, desolate,148, laughing,1, jabbok,7, shuttle,1, arodites,1, michael,15, darkened,19, camest,28, abhorrest,2, beheld,53, looks,5, alpha,4, crieth,17, adders,1, holpen,5, chargeable,5, galatians,1, jezaniah,2, pudens,1, jeremiah,147, coney,2, ditch,6, despitefully,3, sheweth,20, nahaliel,2, sorrows,22, wiser,8, hananeel,4, nicopolis,1, rouse,1, lattice,3, shivers,1, forgettest,2, prophesying,6, shi...\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wc2pre = wc1.map(key_value => s\"${key_value._1},${key_value._2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wc2: Seq[String] = List(onam,4, professed,2, confesseth,3, brink,6, youthful,1, healings,1, sneezed,1, forgotten,46, precious,76, inkhorn,3, exorcists,1, derided,2, eatest,3, lover,4, centurion,21, plentiful,4, pasture,20, sargon,1, speaker,2, terrible,52, lion,104, rate,5, zorites,1, mole,1, lights,10, arimathaea,4, spokes,1, rage,18, submitted,3, engraver,3, ahava,3, ferret,1, snow,24, desolate,148, laughing,1, jabbok,7, shuttle,1, arodites,1, michael,15, darkened,19, camest,28, abhorrest,2, beheld,53, looks,5, alpha,4, crieth,17, adders,1, holpen,5, chargeable,5, galatians,1, jezaniah,2, pudens,1, jeremiah,147, coney,2, ditch,6, despitefully,3, sheweth,20, nahaliel,2, sorrows,22, wiser,8, hananeel,4, nicopolis,1, rouse,1, lattice,3, shivers,1, forgettest,2, prophesying,6, shimrith,1,...\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wc2 = wc2pre.toSeq  // returns a Seq[String]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onam,4\n",
      "professed,2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wc: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[24] at makeRDD at <console>:26\n",
       "res25: Array[Unit] = Array((), ())\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wc = sc.makeRDD(wc2, 1)\n",
    "wc.take(2).map(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the results.\n",
    "\n",
    "> **Note:** If you run the next cell more than once, _delete the output directory first!_ Spark, following Hadoop conventions, won't overwrite an existing directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing output to: output/kjv-wc3\n"
     ]
    }
   ],
   "source": [
    "println(s\"Writing output to: $out\")\n",
    "wc.saveAsTextFile(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "Question: how is the output in `notebooks/output/kjv-wc3` different from the output we generated for _02_WordCount_, `notebooks/output/kjv-wc2`?\n",
    "\n",
    "The `countByValue` function is very convenient for situations like this, but it's not widely used because of its narrow purpose and the risk of exceeding available memory in the job driver."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Try different inputs\n",
    "\n",
    "Change the input `in` and output `out` definitions above to try different files. Does the helper function `toText` need to be changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Sort by word length\n",
    "\n",
    "How would you tell the Scala collections library or the `RDD` API to sort by the length of the words, rather than alphabetically? Look at the sort methods in both libraries. Most of the time, you pass a function that will take as the argument the full \"record\", then you return something to use for sorting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Repeat any of the _02_WordCount_ exercises\n",
    "\n",
    "Some you might try doing in the Scala collection transformations, rather than using `RDD` transformations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
