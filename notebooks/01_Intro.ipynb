{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01: Introduction - A First Look at Spark\n",
    "\n",
    "Our first exercise demonstrates the useful *Spark Shell*, which is a customized version of Scala's REPL (read, eval, print, loop). It allows us to work interactively with our algorithms and data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Local Mode Execution\n",
    "\n",
    "This notebook exploits the fact that we can interactively run Spark expressions, using Scala, Python, or R.\n",
    "\n",
    "Corresponding _script_ file in the GitHub project is\n",
    "[Intro1-script.scala](https://github.com/deanwampler/spark-scala-tutorial/blob/master/src/main/scala/sparktutorial/Intro1-script.scala). It can be run using `spark-shell` in the Spark 2.2.0 distribution:\n",
    "\n",
    "```shell\n",
    "$SPARK_HOME/bin/spark-shell src/main/scala/sparktutorial/Intro1-script.scala\n",
    "```\n",
    "\n",
    "This assumes you're in the directory at the root of the GitHub project. Or, you can start the shell and load this script:\n",
    "\n",
    "```shell\n",
    "$SPARK_HOME/bin/spark-shell\n",
    "...\n",
    "scala> :load src/main/scala/sparktutorial/Intro1-script.scala\n",
    "```\n",
    "\n",
    "Here `scala>` is the Scala \"REPL\" (interpreter) prompt. See the project README.markdown and Tutorial.markdown for more details, including even more options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that several important objects are defined:\n",
    "* `spark` - the entry point object: `org.apache.spark.sql.SparkSession`\n",
    "* `sc` - the pre-2.0 Spark entry point of type `org.apache.spark.SparkContext`, which is created by the `SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://7120f3a07886:4040\n",
       "SparkContext available as 'sc' (version = 3.2.1, master = local[*], app id = local-1655462107208)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@66182f7f\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That link is supposed to open the Spark console, but it doesn't route properly when running in the Docker image. However, the `run.sh` and `run.bat` commands tunnel port 4040, so all you have to do is open <a href=\"http://localhost:4040\" target=\"spark_console\">http://localhost:4040</a>. \n",
    "\n",
    "Note that if you have several notebooks running, they will compete for port 4040, so subsequent notebook \"kernels\" will use ports 4041, 4042, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Class[_ <: org.apache.spark.sql.SparkSession] = class org.apache.spark.sql.SparkSession\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.getClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Class[_ <: org.apache.spark.SparkContext] = class org.apache.spark.SparkContext\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getClass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Spark compiled jobs, like most of the examples in the GitHub project `src` tree, you need to start with logic like the following:\n",
    "\n",
    "```scala\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "\n",
    "val spark = SparkSession.builder.        // builder pattern to construct the session\n",
    "  master(\"local[*]\").                    // run locally on all cores ([*])\n",
    "  appName(\"Console\").\n",
    "  config(\"spark.app.id\", \"Console\").     // to silence Metrics warning\n",
    "  getOrCreate()                          // create it!\n",
    "\n",
    "val sc = spark.sparkContext              // get the SparkContext\n",
    "val sqlContext = spark.sqlContext        // get the old SQLContext, if needed\n",
    "import sqlContext.implicits._            // import useful stuff\n",
    "import org.apache.spark.sql.functions._  // here, import min, max, etc.\n",
    "```\n",
    "\n",
    "The [SparkSession](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession) drives everything else. Before Spark 2.0, the [SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext) was the entry point. You can still extract it from the `SparkSession` as shown near the end, for convenience.\n",
    "\n",
    "The `SparkSession.builder` uses the common _builder pattern_ to construct the session object. Note that value passed to `master`. The setting `local[*]` says run locally on this machine, but take all available CPU cores. Replace `*` with a number to limit this to fewer cores. Just using `local` limits execution to 1 core. There are different arguments you would use for Hadoop, Mesos, Kubernetes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define a read-only variable `input` of type [RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD) by loading the text of the King James Version of the Bible, which has each verse on a line, we then map over the lines converting the text to lower case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "input: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at map at <console>:25\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val input = sc.textFile(\"../data/kjvdat.txt\").  // load a text file, each line is a \"record\" of one field, a string\n",
    "    map(line => line.toLowerCase)            // map each line to lowercase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More accurately, the type of `input` is `RDD[String]` (or `RDD<String>` in Java syntax). Think of it as a \"collection of strings\".\n",
    "\n",
    "> The `data` directory has a `README` that discusses the files present and where they came from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we cache the data in memory for faster, repeated retrieval. You shouldn't always do this, as it's wasteful for data that's simply passed through, but when your workflow will repeatedly reread the data, caching provides performance improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res3: input.type = MapPartitionsRDD[5] at map at <console>:25\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we filter the input for just those verses that mention \"sin\" (recall that the text is now lower case). Then count how many were found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sins: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[61] at filter at <console>:25\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sins = input.filter(line => line.contains(\"sin\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res42: Array[String] = Array(gen|4|7| if thou doest well, shalt thou not be accepted? and if thou doest not well, sin lieth at the door. and unto thee shall be his desire, and thou shalt rule over him.~, gen|10|17| and the hivite, and the arkite, and the sinite,~, gen|12|2| and i will make of thee a great nation, and i will bless thee, and make thy name great; and thou shalt be a blessing:~, gen|13|13| but the men of sodom were wicked and sinners before the lord exceedingly.~, gen|18|20| and the lord said, because the cry of sodom and gomorrah is great, and because their sin is very grievous;~, gen|20|6| and god said unto him in a dream, yea, i know that thou didst this in the integrity of thy heart; for i also withheld thee from sinning against me: therefore suffered i thee not to touc...\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "val sins: org.apache.spark.rdd.RDD[String]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "val count = sins.count()         // How many sins?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, convert the RDD to a Scala collection in the memory for the _driver_ process JVM. That's the JVM behind this notebook interpreting these Spark expressions. If we weren't running in _local_ mode, the driver would delegate some processing to remote JVMs.\n",
    "\n",
    "We'll also loop through the first five lines of the array, printing each one, then we'll do this again with the RDD itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen|4|7| if thou doest well, shalt thou not be accepted? and if thou doest not well, sin lieth at the door. and unto thee shall be his desire, and thou shalt rule over him.~\n",
      "gen|10|17| and the hivite, and the arkite, and the sinite,~\n",
      "gen|12|2| and i will make of thee a great nation, and i will bless thee, and make thy name great; and thou shalt be a blessing:~\n",
      "gen|13|13| but the men of sodom were wicked and sinners before the lord exceedingly.~\n",
      "gen|18|20| and the lord said, because the cry of sodom and gomorrah is great, and because their sin is very grievous;~\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array: Array[String] = Array(gen|4|7| if thou doest well, shalt thou not be accepted? and if thou doest not well, sin lieth at the door. and unto thee shall be his desire, and thou shalt rule over him.~, gen|10|17| and the hivite, and the arkite, and the sinite,~, gen|12|2| and i will make of thee a great nation, and i will bless thee, and make thy name great; and thou shalt be a blessing:~, gen|13|13| but the men of sodom were wicked and sinners before the lord exceedingly.~, gen|18|20| and the lord said, because the cry of sodom and gomorrah is great, and because their sin is very grievous;~, gen|20|6| and god said unto him in a dream, yea, i know that thou didst this in the integrity of thy heart; for i also withheld thee from sinning against me: therefore suffered i thee not to touc...\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val array = sins.collect()       // Convert the RDD into a collection (array)\n",
    "array.take(5).foreach(println)  // Take the first 5, loop through them, and print them 1 per line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen|4|7| if thou doest well, shalt thou not be accepted? and if thou doest not well, sin lieth at the door. and unto thee shall be his desire, and thou shalt rule over him.~\n",
      "gen|10|17| and the hivite, and the arkite, and the sinite,~\n",
      "gen|12|2| and i will make of thee a great nation, and i will bless thee, and make thy name great; and thou shalt be a blessing:~\n",
      "gen|13|13| but the men of sodom were wicked and sinners before the lord exceedingly.~\n",
      "gen|18|20| and the lord said, because the cry of sodom and gomorrah is great, and because their sin is very grievous;~\n"
     ]
    }
   ],
   "source": [
    "sins.take(5).foreach(println)   // ... but we don't have to \"collect\" first;\n",
    "                                 // we can just use foreach on the RDD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to a new topic, you can define *functions* as *values*. Here we create a separate filter function that we pass as an argument to the filter method. Previously we used an *anonymous function*. Note that `filterFunc` is a value that's a function of type `String` to `Boolean`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following more concise form is equivalent, due to *type inference* of the argument's type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "filterFunc: String => Boolean = $Lambda$2917/0x0000000840fcf040@3ba94801\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filterFunc: String => Boolean =\n",
    "    s => s.contains(\"god\") || s.contains(\"christ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the filter to find all the `sin` verses that also mention God or Christ, then count them. Note that this time, we drop the \"punctuation\" in the first line (the comment shows what we dropped), and we drop the parentheses after \"count\". Parentheses can be omitted when methods take no arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sinsPlusGodOrChrist: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[7] at filter at <console>:25\n",
       "countPlusGodOrChrist: Long = 240\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sinsPlusGodOrChrist  = sins filter filterFunc // same as: sins.filter(filterFunc)\n",
    "val countPlusGodOrChrist = sinsPlusGodOrChrist.count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's do _Word Count_, where we load a corpus of documents, tokenize them into words and count the occurrences of all the words.\n",
    "\n",
    "First, we'll define a helper method to look at the data. We need to import the RDD type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.rdd.RDD\n",
       "peek: (rdd: org.apache.spark.rdd.RDD[_], n: Int)Unit\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.rdd.RDD\n",
    "\n",
    "def peek(rdd: RDD[_], n: Int = 10): Unit = {\n",
    "  println(\"RDD type signature: \"+rdd+\"\\n\")\n",
    "  println(\"=====================\")\n",
    "  rdd.take(n).foreach(println)\n",
    "  println(\"=====================\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the type signature `RDD[_]`, the `_` means \"any type\". In other words, we don't care what records this `RDD` is holding, because we're just going to call `toString` on each one. The second argument `n` is the number of records to print. It has a default value of `10`, which means if the caller doesn't provide this argument, we'll print `10` records.\n",
    "\n",
    "The `peek` function prints the type of the `RDD` by calling `toString` on it (effectively). Then it takes the first `n` records, loops through them, and prints each one on a line.\n",
    "\n",
    "Let's use `peek` to remind ourselves what the `input` value is. For this and the next few lines, I'll put in the `scala>` prompt, followed by the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[5] at map at <console>:25\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD type signature: MapPartitionsRDD[5] at map at <console>:25\n",
      "\n",
      "=====================\n",
      "gen|1|1| in the beginning god created the heaven and the earth.~\n",
      "gen|1|2| and the earth was without form, and void; and darkness was upon the face of the deep. and the spirit of god moved upon the face of the waters.~\n",
      "gen|1|3| and god said, let there be light: and there was light.~\n",
      "gen|1|4| and god saw the light, that it was good: and god divided the light from the darkness.~\n",
      "gen|1|5| and god called the light day, and the darkness he called night. and the evening and the morning were the first day.~\n",
      "gen|1|6| and god said, let there be a firmament in the midst of the waters, and let it divide the waters from the waters.~\n",
      "gen|1|7| and god made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so.~\n",
      "gen|1|8| and god called the firmament heaven. and the evening and the morning were the second day.~\n",
      "gen|1|9| and god said, let the waters under the heaven be gathered together unto one place, and let the dry land appear: and it was so.~\n",
      "gen|1|10| and god called the dry land earth; and the gathering together of the waters called he seas: and god saw that it was good.~\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "peek(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `input` is a subtype of `RDD` called `MapPartitionsRDD`. and the `RDD[String]` means the \"records\" are just strings. You might confirm for yourself that the lines shown by `peek(input)` match the input data file.\n",
    "\n",
    "Now, let's split each line into words. We'll treat any run of characters that don't include alphanumeric characters as the \"delimiter\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD type signature: MapPartitionsRDD[8] at flatMap at <console>:26\n",
      "\n",
      "=====================\n",
      "gen\n",
      "in\n",
      "the\n",
      "beginning\n",
      "god\n",
      "created\n",
      "the\n",
      "heaven\n",
      "and\n",
      "the\n",
      "=====================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "words: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[8] at flatMap at <console>:26\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val words = input.flatMap(line => line.split(\"\"\"[^\\p{IsAlphabetic}]+\"\"\"))\n",
    "peek(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does the output make sense to you? The type of the `RDD` hasn't changed, but the records are now individual words.\n",
    "\n",
    "Now let's use our friend from SQL, `GROUPBY`, where we use the words as the \"keys\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD type signature: ShuffledRDD[10] at groupBy at <console>:26\n",
      "\n",
      "=====================\n",
      "(winefat,CompactBuffer(winefat, winefat))\n",
      "(honeycomb,CompactBuffer(honeycomb, honeycomb, honeycomb, honeycomb, honeycomb, honeycomb, honeycomb, honeycomb, honeycomb))\n",
      "(bone,CompactBuffer(bone, bone, bone, bone, bone, bone, bone, bone, bone, bone, bone, bone, bone, bone, bone, bone, bone, bone, bone))\n",
      "(glorifying,CompactBuffer(glorifying, glorifying, glorifying))\n",
      "(nobleman,CompactBuffer(nobleman, nobleman, nobleman))\n",
      "(hodaviah,CompactBuffer(hodaviah, hodaviah, hodaviah))\n",
      "(raphu,CompactBuffer(raphu))\n",
      "(hem,CompactBuffer(hem, hem, hem, hem, hem, hem, hem))\n",
      "(onyx,CompactBuffer(onyx, onyx, onyx, onyx, onyx, onyx, onyx, onyx, onyx, onyx, onyx))\n",
      "(pigeon,CompactBuffer(pigeon, pigeon))\n",
      "=====================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordGroups: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[10] at groupBy at <console>:26\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordGroups = words.groupBy(word => word)\n",
    "peek(wordGroups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the records are now two-element `Tuples`: `(String, Iterable[String])`, where `Iterable` is a Scala abstraction for an underlying, sequential collection. We see that these iterables are actually `CompactBuffers`, a Spark collection that wraps an array of objects. Note that these buffers just hold repeated occurrences of the corresponding keys. This is wasteful, especially at scale! We'll learn a better way to do this calculation shortly.\n",
    "\n",
    "Finally, let's compute the size of each `CompactBuffer`, which completes the calculation of how many occurrences are there for each word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD type signature: MapPartitionsRDD[8] at map at <console>:36\n",
      "\n",
      "=====================\n",
      "(winefat,2)\n",
      "(honeycomb,9)\n",
      "(bone,19)\n",
      "(glorifying,3)\n",
      "(nobleman,3)\n",
      "(hodaviah,3)\n",
      "(raphu,1)\n",
      "(hem,7)\n",
      "(onyx,11)\n",
      "(pigeon,2)\n",
      "=====================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordCounts1 = MapPartitionsRDD[8] at map at <console>:36\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[8] at map at <console>:36"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCounts1 = wordGroups.map( word_group => (word_group._1, word_group._2.size))\n",
    "peek(wordCounts1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the function passed to `map` expects a single two-element `Tuple` argument. We extract the two elements using the `_1` and `_2` methods. (Tuples index from 1, rather than 0, following historical convention.) The type of `wordCounts1` is `RDD[(String,Int)]`.\n",
    "\n",
    "There is a more concise syntax we can use for the method, which exploits _pattern matching_ to break up the tuple into its constituents, which are then assigned to the value names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD type signature: MapPartitionsRDD[11] at map at <console>:26\n",
      "\n",
      "=====================\n",
      "(winefat,2)\n",
      "(honeycomb,9)\n",
      "(bone,19)\n",
      "(glorifying,3)\n",
      "(nobleman,3)\n",
      "(hodaviah,3)\n",
      "(raphu,1)\n",
      "(hem,7)\n",
      "(onyx,11)\n",
      "(pigeon,2)\n",
      "=====================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordCounts2: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[11] at map at <console>:26\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCounts2 = wordGroups.map{ case (word, group) => (word, group.size) }\n",
    "peek(wordCounts2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are exactly the same.\n",
    "\n",
    "But there is actually an even easier way. Note that we aren't modifying the\n",
    "_keys_ (the words), so we can use a convenience function `mapValues`, where only\n",
    "the value part (second tuple element) is passed to the anonymous function and\n",
    "the keys are retained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD type signature: MapPartitionsRDD[12] at mapValues at <console>:26\n",
      "\n",
      "=====================\n",
      "(winefat,2)\n",
      "(honeycomb,9)\n",
      "(bone,19)\n",
      "(glorifying,3)\n",
      "(nobleman,3)\n",
      "(hodaviah,3)\n",
      "(raphu,1)\n",
      "(hem,7)\n",
      "(onyx,11)\n",
      "(pigeon,2)\n",
      "=====================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "wordCounts3: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[12] at mapValues at <console>:26\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordCounts3 = wordGroups.mapValues(group => group.size)\n",
    "peek(wordCounts3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordrank: org.apache.spark.rdd.RDD[(String, Int)] = MapPartitionsRDD[39] at sortBy at <console>:25\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordrank = wordCounts3.sortBy{case (k,v)=> -v}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD type signature: MapPartitionsRDD[29] at sortBy at <console>:25\n",
      "\n",
      "=====================\n",
      "(the,63924)\n",
      "(and,51696)\n",
      "(of,34617)\n",
      "(to,13562)\n",
      "(that,12912)\n",
      "(in,12667)\n",
      "(he,10420)\n",
      "(shall,9838)\n",
      "(unto,8997)\n",
      "(for,8971)\n",
      "(i,8854)\n",
      "(his,8473)\n",
      "(a,8177)\n",
      "(lord,7964)\n",
      "(they,7376)\n",
      "(be,7013)\n",
      "(is,6989)\n",
      "(him,6659)\n",
      "(not,6596)\n",
      "(them,6430)\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "peek(wordrank,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "wordrank2: org.apache.spark.rdd.RDD[((String, Int) => Int, (String, Int))] = MapPartitionsRDD[41] at map at <console>:25\n"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val wordrank2 = wordCounts3.map{ case (k,v) => ((k:String,v:Int)=> (-v) , (k,v))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD type signature: ShuffledRDD[58] at sortByKey at <console>:30\n",
      "\n",
      "=====================\n",
      "(63924,the)\n",
      "(51696,and)\n",
      "(34617,of)\n",
      "(13562,to)\n",
      "(12912,that)\n",
      "(12667,in)\n",
      "(10420,he)\n",
      "(9838,shall)\n",
      "(8997,unto)\n",
      "(8971,for)\n",
      "=====================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "word4: org.apache.spark.rdd.RDD[(Int, String)] = MapPartitionsRDD[55] at map at <console>:27\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val word4 = wordCounts3.map{case (k,v)=> (v,k)}\n",
    "peek(word4.sortByKey(false))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "25: error: value sortByKey is not a member of org.apache.spark.rdd.RDD[((String, Int) => Int, (String, Int))]",
     "output_type": "error",
     "traceback": [
      "<console>:25: error: value sortByKey is not a member of org.apache.spark.rdd.RDD[((String, Int) => Int, (String, Int))]",
      "       val workrand3 = wordrank2.sortByKey()",
      "                                 ^",
      ""
     ]
    }
   ],
   "source": [
    "val workrand3 = wordrank2.sortByKey()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's save the results to the file system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException: Throwable = null\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wordCounts3.saveAsTextFile(\"output/kjv-wc-groupby\")  // \"output\" is a subdirectory of the \"notebooks\" directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look in the directory `output/kjv-wc-groupby` (use the Jupyter file browser), you'll see three files:\n",
    "\n",
    "* `_SUCCESS`\n",
    "* `part-00000`\n",
    "* `part-00001`\n",
    "\n",
    "The `_SUCCESS` file is empty. It's a marker used by the Hadoop File I/O libraries (which Spark uses) to signal to waiting processes that the file output has completed. The other two files each hold a _partition_ of the data. In this case, we had two partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're done, but let's finish by noting that a non-script program should shutdown gracefully by calling `spark.stop()`. However, we don't need to do so here, because both our configured `console` environment for local execution and `spark-shell` do this for us:\n",
    "\n",
    "```scala\n",
    "// sc.stop()\n",
    "```\n",
    "\n",
    "If you exit the REPL immediately, this will happen implicitly. Still, it's a good practice to always call `stop`. Don't do this now; we want to keep the session alive..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Spark Web Console\n",
    "\n",
    "When you have a `SparkContext` running, it provides a web UI with very useful information about how your job is mapped to JVM tasks, metrics about execution, etc.\n",
    "\n",
    "Before we finish this exercise, visit the Spark driver UI at <a href=\"http://localhost:4040\" target=\"spark-driver\">http://localhost:4040</a>. You will find this console very useful for learning Spark internals and when debugging problems, such as performance bottlenecks. \n",
    "\n",
    "> **Tips:** \n",
    "> 1. If you can't get to this UI, it either means you aren't tunneling port 4040 out of the Docker container or no Spark drivers are currently running!\n",
    "> 2. If you have two or more notebooks running, subsequent notebooks will use port 4041, 4042, etc. for the driver UIs. However, these ports aren't tunneled by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "Solutions for some of the suggested exercises are provided in the [src/main/scala/sparktutorial/solns](https://github.com/deanwampler/spark-scala-tutorial/tree/master/src/main/scala/sparktutorial/solns) directory (athough not for this notebook's suggestions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Try Different Filters\n",
    "\n",
    "The filter function could match on a regular expression, for example. Note also the line format in the input text files for the bible `book|chapter#|verse#|verse_text`. It would be easy to filter on the book of the bible, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Try different sacred texts in the \"data\" directory\n",
    "\n",
    "You can also download other texts from http://www.sacred-texts.com/, or just use any other texts you have. Are character sets like UTF-16 handled well, e.g., the Hebrew text in `../data/t3utf.dat`?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
